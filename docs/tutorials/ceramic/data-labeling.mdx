---
title: Label data for LLM training
description: Create a labeled dataset for LLM training with event storage and OrbisDB.
---

In the ever-evolving world of machine learning, fine-tuning Large Language Models (LLMs) has become
a crucial step in achieving optimal performance for specific tasks. One of the most time-consuming
aspects of this process is data labeling, and facilitating this step with care and attention to
detail directly impacts your model’s success.

As individuals and teams collaborate on these projects, tracking the origin and evolution of
datasets becomes increasingly important. For example, teams working on LLM pipelines care about:

- **Quality Assurance**: Being able to easily identify the source of high-quality datasets and
  potential areas for improvement.
- **Accountability**: Fostering a sense of ownership and responsibility among team members for the
  datasets they create.
- **Collaboration**: Enhancing teamwork by providing clear visibility into each member's
  contributions.
- **Data Provenance**: Maintaining strong, reliable proof of data origination when sourcing data
  from third parties.
- **Auditing and Compliance**: Maintaining a clear record of dataset origins (often necessary for
  industries with strict regulatory requirements).
- **Iterative Improvement**: Track the evolution of datasets over time, enabling teams to analyze
  and refine their labeling processes.

Given these desires, it’s easy to see how tools that provide strong provenance and data lineage
characteristics might benefit ML pipeline architects. This is where Recall’s Event Storage
capabilities come into play.

## Recall event storage for data labeling

Recall’s Event Storage capabilities feature Ceramic - a decentralized data network built on
verifiable event streams on top of which relational databases, private storage, and event-driven
capabilities can be built. A key characteristic of Ceramic is its verifiable data integrity and
lineage. Here’s how this relates to data labeling:

- All data (such as labeled datasets) written to Ceramic are "owned" by an account controlled by a
  sovereign decentralized identifier. These identifiers can be owned by an Ethereum address or
  instantiated to extend Ed25519 and Secp256k1 public-key pairs.
- Once records (or "streams") are created, only the controlling account can continue to make updates
  to that data (or by granting an application temporary access to make updates on behalf of the
  controlling account).
- Streams preserve the history of edits made, making historical traversal transparent.
- Finally, given Ceramic’s private data and inter-node synchronization capabilities, marketplaces
  for labeled data can be built on this infrastructure, enabling dataset producers to allow read
  access in exchange for monetary value.

With these qualities and reasoning in mind, we’ve written this technical guide to walk through one
example of how to facilitate data labeling into Recall Event Storage.

### Key components

This demo relies on the following frameworks and providers to support our data labeling flow:

**OrbisDB**

The previous section mentioned that databases can be built on Ceramic’s event streams.
[OrbisDB](https://useorbis.com/) is one of those implementations, offering a relational database
interface, and features like plugins, a developer UI, and even a shared hosted instance for
experimentation.

Given the highly relational nature of dataset labeling, we will use OrbisDB (and its SDK) to write
our labeled datasets to Ceramic.

**Label Studio**

[Label Studio](https://labelstud.io/) is a flexible, open-source data labeling framework that can be
used to prepare training data for computer vision, natural language processing (NLP), speech, voice,
and video models. This demo will showcase a "Text Classification" use case, a subset of NLP.

**Privy Wallet**

Finally, this demo assumes that many data labelers do not have Ethereum wallets and may opt to log
in with a social or email platform instead. Since Privy spins up a wallet for users who log in with
social platforms (while also allowing people to authenticate with their wallet if they have one), we
can extend Privy to authenticate on Ceramic, create a browser session, and yield a DID to claim
ownership to our labeled data.

## Set Up Your Environment

This guide uses a modified version of the Label Studio codebase (accounting for the Privy and
OrbisDB integrations). To get started, clone the following repository:

```bash
git clone https://github.com/ceramicstudio/orbis-label-studio
```

The next steps will be partitioned into three parts accounting for our OrbisDB, Privy, and Label
Studio setups.

### OrbisDB Setup

Label Studio operates using a Python backend and a React-based frontend. Our OrbisDB SDK is
integrated into the frontend piece of our architecture housed in the `web` directory. Go ahead and
enter that directory and create a copy of the example environment file:

```bash
cd web
cp .env.example .env
```

As mentioned above, one of the nice features that makes OrbisDB easy to work with is its hosted
interface and shared node instance that developers can use to experiment and iterate quickly. This
can be found at the [OrbisDB Studio](https://studio.useorbis.com/). Create a free account if you do
not yet have one

Once signed in, the studio will default to the `Contexts` tab at the top. On the right-hand side,
you will see the shared node endpoints (already provided for you in your env file), and your
environment ID:

![orbisdb context page](/img/tutorials/data-labeling/orbisdb-context.png)

Go ahead and assign that value to `ENV_ID` in your new `.env` file.

Next, set up a context. These help developers segment their data models and usage based on the
applications they are meant for. Create a new content (you can call it "data-labeling" if you'd
like), and assign the resulting string to `CONTEXT_ID` in your `.env` file.

Finally, create a table to accommodate the labeled data. As mentioned above, you will label data
using the sentiment analysis interface. This guide
uses [this dataset from Hugging Face](https://huggingface.co/datasets/LabelStudio/IMDB_Sample_100)
found in
this [dataset file](https://github.com/ceramicstudio/orbis-label-studio/blob/main/dataset.csv) in
the repository. The Label Studio framework will transform this data as we label it, applying the
human-assigned sentiment analysis and other values (such as when the data was labeled, who it was
labeled by, and so on).

Back in your Orbis Studio view, select the "Model Builder" tab at the top and create a new model
named "labeled_data" using
this [table definition](https://github.com/ceramicstudio/orbis-label-studio/blob/main/models/tables.sql) (starting
with `id`):

![orbisdb model creation](/img/tutorials/data-labeling/model-creation.png)

After clicking "Create Model" assign the result to `TABLE_ID` in your `.env` file. This will be
referenced by the OrbisDB SDK when adding new rows to our dataset.

### Privy Setup

To start setting up Privy, log into your [Privy Dashboard](https://dashboard.privy.io/) (or set up
an account for free) and create a new app. Once you have a new app set up, click on the app. Under
"Getting Started checklist" you will see a box called "Set user login methods". Click that option,
and select the "Socials" tab at the top of the "Login Methods" page. Select the "Google" and "X
(Twitter)" options (Privy allows you to use their default OAuth credentials for these):

![privy setup screen](/img/tutorials/data-labeling/privy-setup.png)

Finally, go back to the "Getting started checklist" and select "Settings" under "Retrieve API keys".
Copy the "App ID" value into your `.env` file by assigning it to `PRIVY_ID`.

### Label Studio Local Setup

Since we are running a modified version of Label Studio, we will install it for local development
and initiate a static asset migration to run it locally (you will need Python v3.11.8 installed
locally).

Open a new terminal in the root of this directory and run:

```bash
# Install all package dependencies
pip install poetry
poetry install
# Set up shell
poetry shell
# Run database migrations
python label_studio/manage.py migrate
python label_studio/manage.py collectstatic
```

You are now ready to run the demo!

## Running the Application

To begin, start it up locally using the following command:

```bash
# Start the server in development mode at http://localhost:8080
python label_studio/manage.py runserver
```

Navigate to [`http://localhost:8080`](http://localhost:8080) in your browser. You will be prompted
to log in using Label Studio's email+password authentication (kept in for now, but not needed in
future iterations since we are using Privy). Once you've signed up with a new email and logged in,
create a new project by clicking "Create Project":

![labeling create project](/img/tutorials/data-labeling/create-project.png)

Choose a name and description of your liking, and go to the "Data Import" tab.

Click "Upload Files" and
select [this dataset](https://github.com/ceramicstudio/orbis-label-studio/blob/main/dataset.csv) from
your filesystem.

Select "Treat CSV as List of Tasks" as the next option.

Finally, select the "Labeling Setup" tab at the top, the "Natural Language Processing" option from
the side, and the "Text Classification" option. Click "Save" after these steps in the upper
right-hand:

![labeling text classification](/img/tutorials/data-labeling/text-classification.png)

### Authenticate with Privy

Select the hamburger menu in the upper left-hand side and click "Log In". Go ahead and use Google as
your sign-in method. You will be prompted to sign a secondary message:

<div style={{ display: "flex", justifyContent: "center" }}>
  ![ceramic authentication with privy](/img/tutorials/data-labeling/ceramic-auth.png)
</div>

This action will authenticate you to Ceramic and create a secure browser session that the front end
will use on your behalf when you submit your labeled dataset.

### Labeling Data

Back in your project view, start labeling each row. You can customize your UI when in the labeling
view to see the review you’re marking using one of the three given options:

![labeling data](/img/tutorials/data-labeling/labeling-data.png)

Label as many or as few as you'd like. Once ready, return to the project view for this project.

### Saving to Ceramic

Once back in your project view for this project, select the "Export" button on the upper right-hand
side.

The table we configured in Orbis earlier conforms to the `JSON_MIN` data format, so select that from
the list of options:

<div style={{ display: "flex", justifyContent: "center" }}>
  ![exporting labeled data](/img/tutorials/data-labeling/export-data.png)
</div>

Finally, select the "Save to Ceramic" button at the bottom. This will automatically write our
labeled data to Ceramic via OrbisDB (which
occurs [in this component](https://github.com/ceramicstudio/orbis-label-studio/blob/main/web/apps/labelstudio/src/pages/ExportPage/ExportPage.jsx)).
You can reference the code for how the OrbisDB SDK is being leveraged to facilitate this behavior:

```jsx
// obtain and parse the JSON-MIN data
const json = await blob.text();
const rows = JSON.parse(json);

// make sure authenticated account is connected
await orbis.getConnectedUser();

for (const row of rows) {
  const updatequery = await orbis
    .insert(process.env.TABLE_ID) // this is the table we created earlier
    .value(row)
    .context(process.env.CONTEXT_ID) // this is the context we created
    .run();
}
```

### Viewing Your Data in the Orbis Studio

Navigate back to your OrbisDB Studio view where you can select the "Data" tab at the top, and your
"labeled_data" table from the left-hand side:

![viewing labeled data](/img/tutorials/data-labeling/viewing-labeled-data.png)

## Next Steps

While this guide walks through a simple implementation of text classification data labeling using
Recall Event Storage, there are many other places within the LLM pipeline where Ceramic could be
leveraged. While we facilitated the writing portion of this process above, an obvious next step
would be to spin up a service to read and consume these datasets, and perhaps apply weighted logic
based on the expertise behind who created them.

Are you interested in leveraging Recall for your data labeling needs? Did you find this guide
helpful? We’d love to hear from you!
