---
title: AI SDK
description: Using Recall with the AI SDK framework
---

This guide shows you how to integrate the Recall Agent Toolkit with Vercel's AI SDK to build web
applications with Recall-powered agents.

## Overview

[Vercel's AI SDK](https://sdk.vercel.ai/docs) is a powerful toolkit for building AI-powered
applications, especially in Next.js applications. It provides components and utilities for building
user interfaces that interact with AI models.

By integrating Recall Agent Toolkit with the AI SDK, you can:

- Build Next.js and React applications with Recall-powered AI functionality
- Create UI components that interact with Recall storage
- Implement streaming responses for better UX
- Enable tool usage for AI agents directly in your web app
- Handle multi-turn conversations with persistent memory

## Installation

<Steps>

<Step>

### Install required packages

```package-install
npm install @recallnet/agent-toolkit ai @ai-sdk/openai react
```

If you're using Next.js:

```package-install
npm install next
```

</Step>

<Step>

### Set up environment variables

Create a `.env.local` file in your project root:

```
RECALL_PRIVATE_KEY=your_recall_private_key
OPENAI_API_KEY=your_openai_api_key
```

</Step>

<Step>

### Configure Next.js environment variables

In `next.config.js`, add:

```js
module.exports = {
  env: {
    RECALL_PRIVATE_KEY: process.env.RECALL_PRIVATE_KEY,
    OPENAI_API_KEY: process.env.OPENAI_API_KEY,
  },
};
```

</Step>

</Steps>

## Basic integration

Here's how to integrate Recall with the AI SDK in a Next.js application:

```typescript title="app/api/chat/route.ts"
import { OpenAI } from "@ai-sdk/openai";
import { RecallAgentToolkit } from "@recallnet/agent-toolkit/ai-sdk";
import { Message, StreamingTextResponse } from "ai";

// Create the OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

// Create the Recall toolkit
const toolkit = new RecallAgentToolkit({
  privateKey: process.env.RECALL_PRIVATE_KEY!,
  configuration: {
    actions: {
      bucket: { read: true, write: true },
    },
    context: {},
  },
});

// Get the tools
const tools = toolkit.getTools();

export async function POST(req: Request) {
  const { messages } = await req.json();

  // Add system message if not present
  if (!messages.find((m: Message) => m.role === "system")) {
    messages.unshift({
      role: "system",
      content:
        "You are a helpful assistant with access to Recall storage. Use this to remember important information for the user.",
    });
  }

  // Generate a response with the AI SDK
  const response = await openai.chat({
    model: "gpt-4o",
    messages,
    tools,
  });

  // Create a streaming response
  return new StreamingTextResponse(response.content);
}
```

And the client-side React component:

```tsx title="app/page.tsx"
"use client";

import { useChat } from "ai/react";

export default function ChatPage() {
  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat();

  return (
    <div className="stretch mx-auto flex w-full max-w-md flex-col py-24">
      <h1 className="mb-4 text-2xl font-bold">Recall-Powered Chat</h1>

      {messages.length > 0 ? (
        messages.map((m) => (
          <div
            key={m.id}
            className={`mb-4 whitespace-pre-wrap ${
              m.role === "user" ? "rounded bg-blue-100 p-2" : "rounded bg-gray-100 p-2"
            }`}
          >
            <b>{m.role === "user" ? "You: " : "AI: "}</b>
            {m.content}
          </div>
        ))
      ) : (
        <div className="text-gray-500">Send a message to get started</div>
      )}

      <form onSubmit={handleSubmit} className="mt-4 flex gap-2">
        <input
          className="flex-1 rounded border border-gray-300 p-2"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
        <button type="submit" className="rounded bg-blue-500 p-2 text-white" disabled={isLoading}>
          Send
        </button>
      </form>
    </div>
  );
}
```

This basic example sets up a chat interface that can use Recall for storage via tools.

## Function calling

For more advanced implementations, you'll need to handle function calls properly. Here's how to do
that:

```typescript title="app/api/chat/route.ts"
import { OpenAI } from "@ai-sdk/openai";
import { RecallAgentToolkit } from "@recallnet/agent-toolkit/ai-sdk";
import { Message, StreamingTextResponse } from "ai";

// Create the OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

// Create the Recall toolkit
const toolkit = new RecallAgentToolkit({
  privateKey: process.env.RECALL_PRIVATE_KEY!,
  configuration: {
    actions: {
      bucket: { read: true, write: true },
    },
  },
});

// Get the tools
const tools = toolkit.getTools();

// Keep track of conversation buckets
const conversationBuckets = new Map<string, string>();

export async function POST(req: Request) {
  const { messages, id: conversationId } = await req.json();

  // Create a bucket for this conversation if it doesn't exist
  if (!conversationBuckets.has(conversationId)) {
    try {
      const result = await toolkit.run("get_or_create_bucket", {
        bucketAlias: `conversation-${conversationId}`,
      });
      conversationBuckets.set(conversationId, result.bucket);
    } catch (error) {
      console.error("Failed to create bucket:", error);
    }
  }

  // Add system message if not present
  if (!messages.find((m: Message) => m.role === "system")) {
    messages.unshift({
      role: "system",
      content: `You are a helpful assistant with access to Recall storage.
      Use storage to remember important information between conversations.
      The user has an assigned conversation bucket that you can use for storage.
      Always consider the contents of that bucket as the user's memory.`,
    });
  }

  // Generate a response with OpenAI
  const response = await openai.chat({
    model: "gpt-4o",
    messages,
    tools,
    tool_choice: "auto",
  });

  // Check if the response includes tool calls
  const toolCalls = response.tool_calls;

  if (toolCalls && toolCalls.length > 0) {
    // Process tool calls one by one
    const results = await Promise.all(
      toolCalls.map(async (toolCall) => {
        try {
          const toolName = toolCall.name;
          const toolArgs = toolCall.args;

          // Execute the tool using the toolkit
          const result = await toolkit.run(toolName, toolArgs);

          return {
            tool_call_id: toolCall.id,
            name: toolName,
            result,
          };
        } catch (error) {
          console.error(`Error executing tool ${toolCall.name}:`, error);
          return {
            tool_call_id: toolCall.id,
            name: toolCall.name,
            error: error.message,
          };
        }
      })
    );

    // Create an updated messages array with the tool results
    const updatedMessages = [
      ...messages,
      {
        role: "assistant",
        content: null,
        tool_calls: toolCalls.map((tc) => ({
          id: tc.id,
          type: "function",
          function: {
            name: tc.name,
            arguments: JSON.stringify(tc.args),
          },
        })),
      },
      ...results.map((result) => ({
        role: "tool",
        tool_call_id: result.tool_call_id,
        content: result.error ? `Error: ${result.error}` : JSON.stringify(result.result),
      })),
    ];

    // Get final assistant response
    const finalResponse = await openai.chat({
      model: "gpt-4o",
      messages: updatedMessages,
    });

    // Return the final response
    return new StreamingTextResponse(finalResponse.content);
  }

  // If no tool calls, just return the original response
  return new StreamingTextResponse(response.content);
}
```

## Memory management

For maintaining conversation context across sessions:

```typescript
// app/api/chat/route.ts
import { OpenAI } from "@ai-sdk/openai";
import { RecallAgentToolkit } from "@recallnet/agent-toolkit/ai-sdk";
import { Message, StreamingTextResponse } from "ai";

// Create the client and toolkit
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

const toolkit = new RecallAgentToolkit({
  privateKey: process.env.RECALL_PRIVATE_KEY!,
  configuration: {
    actions: {
      bucket: { read: true, write: true },
    },
  },
});

const tools = toolkit.getTools();

// Handle conversation memory
async function loadConversationMemory(conversationId: string, recentMessages: Message[]) {
  try {
    // Create or get bucket for this conversation
    const bucketResult = await toolkit.run("get_or_create_bucket", {
      bucketAlias: `memory-${conversationId}`,
    });

    // Try to load past memory
    try {
      const memoryResult = await toolkit.run("get_object", {
        bucket: bucketResult.bucket,
        key: "conversation-summary",
      });

      const memoryData = JSON.parse(memoryResult.value);

      // Return both the memory and the recent messages
      return {
        memory: memoryData.summary,
        bucketId: bucketResult.bucket,
      };
    } catch (error) {
      // No memory exists yet, which is fine
      return {
        memory: "No previous conversation found.",
        bucketId: bucketResult.bucket,
      };
    }
  } catch (error) {
    console.error("Error loading memory:", error);
    return {
      memory: "Failed to load previous conversation.",
      bucketId: null,
    };
  }
}

async function saveConversationMemory(bucketId: string, messages: Message[]) {
  if (!bucketId) return;

  try {
    // Create a summary of the conversation
    const summaryResponse = await openai.chat({
      model: "gpt-4o",
      messages: [
        {
          role: "system",
          content:
            "Summarize the following conversation in a concise paragraph that captures the key points, important information, and any decisions or actions taken.",
        },
        ...messages,
      ],
    });

    // Save the summary and the last few messages
    await toolkit.run("add_object", {
      bucket: bucketId,
      key: "conversation-summary",
      data: JSON.stringify({
        summary: summaryResponse.content,
        lastUpdated: new Date().toISOString(),
        recentMessages: messages.slice(-5),
      }),
      overwrite: true,
    });
  } catch (error) {
    console.error("Error saving memory:", error);
  }
}

export async function POST(req: Request) {
  const { messages, id: conversationId = "default" } = await req.json();

  // Load conversation memory
  const { memory, bucketId } = await loadConversationMemory(conversationId, messages);

  // Construct messages with memory context
  const messagesWithMemory = [
    {
      role: "system",
      content: `You are a helpful assistant with access to Recall storage and memory of past conversations.

Memory from previous conversations:
${memory}

Use this context to provide more consistent and personalized responses. If the user refers to something from a past conversation, use this memory to understand the reference.

You can also use Recall storage tools to save and retrieve information as needed.`,
    },
    ...messages,
  ];

  // Generate response
  const response = await openai.chat({
    model: "gpt-4o",
    messages: messagesWithMemory,
    tools,
  });

  // Save the conversation after getting a response
  if (bucketId) {
    // We save it in the background to not block the response
    saveConversationMemory(bucketId, [
      ...messages,
      { role: "assistant", content: response.content },
    ]).catch(console.error);
  }

  return new StreamingTextResponse(response.content);
}
```

## Next.js App Router integration

For a more complete Next.js application with the App Router:

<Tabs items={["Full App Structure", "Route Handler", "UI Component"]}>
  <Tab value="Full App Structure">

    ```
    my-recall-app/
    ‚îú‚îÄ‚îÄ app/
    ‚îÇ   ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat/
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ route.ts   # API route handler
    ‚îÇ   ‚îú‚îÄ‚îÄ chat/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx       # Chat page component
    ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx         # App layout
    ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx           # Home page
    ‚îú‚îÄ‚îÄ components/
    ‚îÇ   ‚îú‚îÄ‚îÄ chat-interface.tsx # Reusable chat component
    ‚îÇ   ‚îî‚îÄ‚îÄ recall-provider.tsx # Recall context provider
    ‚îú‚îÄ‚îÄ lib/
    ‚îÇ   ‚îú‚îÄ‚îÄ recall.ts          # Recall toolkit setup
    ‚îÇ   ‚îî‚îÄ‚îÄ utils.ts           # Utility functions
    ‚îú‚îÄ‚îÄ .env.local             # Environment variables
    ‚îú‚îÄ‚îÄ next.config.js         # Next.js configuration
    ‚îú‚îÄ‚îÄ package.json           # Dependencies
    ‚îî‚îÄ‚îÄ tsconfig.json          # TypeScript configuration
    ```

  </Tab>
  <Tab value="Route Handler">

    ```typescript
    // app/api/chat/route.ts
    import { OpenAI } from '@ai-sdk/openai';
    import { Message, StreamingTextResponse } from 'ai';
    import { NextRequest, NextResponse } from 'next/server';
    import { getRecallToolkit } from '@/lib/recall';

    export const runtime = 'edge';

    export async function POST(req: NextRequest) {
      try {
        // Get request data
        const { messages, id = 'default' } = await req.json();

        // Initialize the toolkit and client
        const toolkit = getRecallToolkit();
        const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

        // Get tools
        const tools = toolkit.getTools();

        // Prepare system message
        const systemMessage = {
          role: 'system',
          content: `You are a helpful assistant with access to Recall storage. Use Recall to remember important information between conversations.`,
        };

        // Add system message if not present
        const updatedMessages = messages.find((m: Message) => m.role === 'system')
          ? messages
          : [systemMessage, ...messages];

        // Generate response with tool support
        const response = await openai.chat({
          model: 'gpt-4',
          messages: updatedMessages,
          tools,
        });

        // Process any tool calls
        if (response.tool_calls && response.tool_calls.length > 0) {
          const results = await Promise.all(
            response.tool_calls.map(async (toolCall) => {
              try {
                // Execute the tool
                const result = await toolkit.run(toolCall.name, toolCall.args);
                return {
                  tool_call_id: toolCall.id,
                  content: JSON.stringify(result),
                };
              } catch (error) {
                console.error(`Tool error (${toolCall.name}):`, error);
                return {
                  tool_call_id: toolCall.id,
                  content: `Error: ${error.message}`,
                };
              }
            })
          );

          // Create final messages with tool results
          const finalMessages = [
            ...updatedMessages,
            {
              role: 'assistant',
              content: null,
              tool_calls: response.tool_calls.map((tc) => ({
                id: tc.id,
                type: 'function',
                function: {
                  name: tc.name,
                  arguments: JSON.stringify(tc.args),
                },
              })),
            },
            ...results.map((r) => ({
              role: 'tool',
              tool_call_id: r.tool_call_id,
              content: r.content,
            })),
          ];

          // Get final response
          const finalResponse = await openai.chat({
            model: 'gpt-4',
            messages: finalMessages,
          });

          return new StreamingTextResponse(finalResponse.content);
        }

        // If no tool calls, return the original response
        return new StreamingTextResponse(response.content);
      } catch (error) {
        console.error('Chat API error:', error);
        return NextResponse.json(
          { error: 'Internal server error' },
          { status: 500 }
        );
      }
    }
    ```

  </Tab>
  <Tab value="UI Component">

    ```tsx
    // components/chat-interface.tsx
    'use client';

    import { useState, useRef, useEffect } from 'react';
    import { useChat } from 'ai/react';
    import { IoSend } from 'react-icons/io5';

    export default function ChatInterface({ id = 'default' }) {
      const [loading, setLoading] = useState(false);
      const messagesEndRef = useRef<HTMLDivElement>(null);

      // Initialize chat with AI SDK hook
      const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
        id,
        onResponse: () => {
          setLoading(false);
        },
        onError: (error) => {
          console.error('Chat error:', error);
          setLoading(false);
        },
      });

      // Scroll to bottom when messages change
      useEffect(() => {
        if (messagesEndRef.current) {
          messagesEndRef.current.scrollIntoView({ behavior: 'smooth' });
        }
      }, [messages]);

      // Custom submit handler
      const onSubmit = (e: React.FormEvent<HTMLFormElement>) => {
        e.preventDefault();
        if (!input.trim() || isLoading) return;

        setLoading(true);
        handleSubmit(e);
      };

      return (
        <div className="flex flex-col h-[80vh] max-w-3xl mx-auto border border-gray-200 rounded-md overflow-hidden">
          {/* Chat header */}
          <div className="p-4 bg-blue-600 text-white font-medium">
            Recall-Powered Chat Assistant
          </div>

          {/* Chat messages */}
          <div className="flex-1 overflow-y-auto p-4 space-y-4">
            {messages.length === 0 ? (
              <div className="text-gray-500 text-center mt-8">
                üëã Send a message to start the conversation
              </div>
            ) : (
              messages.map((message) => (
                <div
                  key={message.id}
                  className={`flex ${
                    message.role === 'user' ? 'justify-end' : 'justify-start'
                  }`}
                >
                  <div
                    className={`max-w-[80%] p-3 rounded-lg ${
                      message.role === 'user'
                        ? 'bg-blue-500 text-white rounded-tr-none'
                        : 'bg-gray-200 text-gray-800 rounded-tl-none'
                    }`}
                  >
                    <p className="whitespace-pre-wrap">{message.content}</p>
                  </div>
                </div>
              ))
            )}
            <div ref={messagesEndRef} />
          </div>

          {/* Chat input */}
          <form onSubmit={onSubmit} className="border-t border-gray-200 p-4">
            <div className="flex items-center">
              <input
                type="text"
                value={input}
                onChange={handleInputChange}
                placeholder="Type your message..."
                className="flex-1 p-2 border border-gray-300 rounded-l-md focus:outline-none focus:ring-2 focus:ring-blue-500"
                disabled={isLoading}
              />
              <button
                type="submit"
                disabled={isLoading}
                className="bg-blue-600 text-white p-2 rounded-r-md hover:bg-blue-700 transition-colors disabled:bg-blue-400"
              >
                <IoSend />
              </button>
            </div>
          </form>
        </div>
      );
    }
    ```

  </Tab>
</Tabs>

## Recall Agent Toolkit setup

A common pattern is to centralize your Recall toolkit setup:

```typescript title="lib/recall.ts"
import { RecallAgentToolkit } from "@recallnet/agent-toolkit/ai-sdk";

let toolkit: RecallAgentToolkit | null = null;

export function getRecallToolkit() {
  if (!toolkit) {
    toolkit = new RecallAgentToolkit({
      privateKey: process.env.RECALL_PRIVATE_KEY!,
      configuration: {
        actions: {
          account: { read: true },
          bucket: { read: true, write: true },
        },
      },
      middleware: [
        {
          beforeToolCall: (tool, input) => {
            console.log(`Calling ${tool.name} with:`, input);
            return input;
          },
          afterToolCall: (tool, input, output) => {
            console.log(`${tool.name} result:`, output);
            return output;
          },
          onToolError: (tool, input, error) => {
            console.error(`${tool.name} error:`, error);
            throw error;
          },
        },
      ],
    });
  }

  return toolkit;
}
```

## Best practices

When integrating Recall with the AI SDK, follow these best practices:

1. **Server-side execution**: Always run Recall tools on the server side to protect your private key
2. **Error handling**: Implement robust error handling for tool calls and API requests
3. **Loading states**: Show appropriate loading states during tool execution for better UX
4. **Streaming responses**: Use streaming for better user experience
5. **Environment variables**: Keep sensitive keys in environment variables
6. **Middleware for debugging**: Use middleware to log tool calls during development:

   ```typescript
   middleware: [
     {
       beforeToolCall: (tool, input) => {
         console.log(`Calling ${tool.name} with:`, input);
         return input;
       },
       afterToolCall: (tool, input, output) => {
         console.log(`${tool.name} result:`, output);
         return output;
       },
       onToolError: (tool, input, error) => {
         console.error(`${tool.name} error:`, error);
         throw error;
       },
     },
   ];
   ```

7. **Route protection**: Add authentication to protect your API routes if needed

## Troubleshooting

| Issue                                 | Possible cause                    | Solution                                       |
| ------------------------------------- | --------------------------------- | ---------------------------------------------- |
| "Private key not found"               | Missing environment variable      | Ensure RECALL_PRIVATE_KEY is set in .env.local |
| "Cannot read properties of undefined" | Incorrect tool usage              | Verify tool parameters match expected schema   |
| "Error: ECONNREFUSED"                 | Network issue or API service down | Check internet connection and API status       |
| Slow responses                        | Complex tool operations           | Implement proper loading states in UI          |
| "Function not found"                  | Incorrect tool name               | Ensure tool names match exactly                |

## AI SDK helper components

The AI SDK provides several useful React components and hooks:

- `useChat`: Main hook for chat functionality
- `useCompletion`: Hook for single-turn completions
- `Message`: Type for chat messages
- `StreamingTextResponse`: Class for streaming responses

Here's how to use them effectively:

```tsx
"use client";

import { useChat, useCompletion } from "ai/react";

// For chat interfaces
export function ChatComponent() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: "/api/chat",
    initialMessages: [
      { id: "1", role: "system", content: "You are a helpful assistant with Recall storage." },
    ],
  });

  // Your chat UI
}

// For single-turn completions
export function CompletionComponent() {
  const { completion, input, handleInputChange, handleSubmit } = useCompletion({
    api: "/api/completion",
  });

  // Your completion UI
}
```

## Advanced Vercel AI SDK features

Vercel's AI SDK offers advanced features for production applications:

1. **Rate limiting**: Add rate limiting to prevent abuse

   ```typescript
   import { Ratelimit } from "@upstash/ratelimit";
   import { Redis } from "@upstash/redis";

   const ratelimit = new Ratelimit({
     redis: Redis.fromEnv(),
     limiter: Ratelimit.slidingWindow(5, "60 s"),
   });

   export async function POST(req: Request) {
     const ip = req.headers.get("x-forwarded-for") || "anonymous";
     const { success, limit, reset } = await ratelimit.limit(ip);

     if (!success) {
       return new Response("Too many requests", {
         status: 429,
         headers: {
           "X-RateLimit-Limit": limit.toString(),
           "X-RateLimit-Remaining": "0",
           "X-RateLimit-Reset": reset.toString(),
         },
       });
     }

     // Continue with normal request processing
   }
   ```

2. **Caching**: Cache responses for similar queries

   ```typescript
   export async function POST(req: Request) {
     const { messages } = await req.json();
     const cacheKey = JSON.stringify(messages);

     // Check cache
     const cache = await caches.open("ai-responses");
     const cachedResponse = await cache.match(cacheKey);

     if (cachedResponse) {
       return cachedResponse;
     }

     // Generate response
     const response = await openai.chat({
       model: "gpt-4o",
       messages,
       tools,
     });

     // Cache response
     const streamingResponse = new StreamingTextResponse(response.content);
     const clonedResponse = streamingResponse.clone();
     cache.put(cacheKey, clonedResponse);

     return streamingResponse;
   }
   ```

3. **Feedback collection**: Add feedback mechanisms to improve your model

   ```tsx
   function FeedbackButtons({ messageId }: { messageId: string }) {
     const submitFeedback = async (type: "positive" | "negative") => {
       await fetch("/api/feedback", {
         method: "POST",
         body: JSON.stringify({ messageId, type }),
       });
     };

     return (
       <div className="mt-1 flex space-x-2">
         <button
           onClick={() => submitFeedback("positive")}
           className="text-sm text-gray-500 hover:text-green-600"
         >
           üëç Helpful
         </button>
         <button
           onClick={() => submitFeedback("negative")}
           className="text-sm text-gray-500 hover:text-red-600"
         >
           üëé Not helpful
         </button>
       </div>
     );
   }
   ```

## Next steps

- Explore the [core concepts](/agent-toolkit/core-concepts) to better understand the Recall
  capabilities
- Learn about [bucket monitoring](/agent-toolkit/bucket-monitoring) to track agent activity
- Check out the [MCP integration](/mcp) guide for a different approach
- See the [tools reference](/agent-toolkit/tools-reference) for detailed documentation of available
  tools

<Callout type="info">

The Vercel AI SDK is regularly updated with new features. Check the
[official documentation](https://sdk.vercel.ai/docs) for the latest updates.

</Callout>
